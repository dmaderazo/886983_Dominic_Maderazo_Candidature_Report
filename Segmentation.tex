Change detection is a statistical problem concerned with determining when a change has occurred in a stochastic process underlying data generation. 
Consider a game with 2 $D$-sided dice, each with a different probability distribution governed by a parameter $\theta$ such that $\theta_i \neq \theta_j$. A dice is rolled an unknown number of times and changed for the another. 
%This process is repeated $\kappa \leq k_{max}$ number of times, with $\k_{max}$ unknown. 
A record of each of the outcomes of all dice are recorded. Given only the concatenation of all the outcomes, the goal of change point detection is to determine when the dice were changed and the distribution of each die. 
% Consider a game where there are two coins with probability $\theta_1$ and $\theta_2$ of producing heads, with $\theta_1 \neq \theta_2$. One of the coins is flipped and unknown number of times and exchanged for the other, which is also flipped an unknown number of times. 
% The sequence of the flips is recorded in sequence and this data is all that is known, as well as the number of coins.
% In this example, the goal of change detection is to determine at what point the coins were exchanged as well as the estimation for the parameters $\theta_1$ and $\theta_2$. 

% In bioinformatics, it common for the number of change points to be unknown {\color{red}perhaps corresponding to the start and end of putative functional elements}; this is a typical extension of change point detection. 
To generalize the above problem to this context, the data of primary concern are sequences of amino acids or nucleotide bases with an unknown number of segments. 
Instead of only dealing with two dice, there may be $k_{max}$ $D$-sided dice (with outcomes $\{1,\ldots,D\}$) having different biases (i.e that $\theta_i\neq \theta_j$ for $i\neq j$). 
The $i$th die is rolled $C_i$ times until they have used $\kappa\leq k_{max}$ of the dice a total of 
    \begin{equation}
        J = \sum_{i=1}^{\kappa}C_i
    \end{equation}
number of times, with $k_{max}$ unknown. Given only the concatenation of the outcomes $S = s_2s_2\ldots s_J$, the goal of the change point problem is to be able to determine the points where the opponent has switched dice. In the context of biopolymer molecules the sequences of interest may be nucleotides $s_i \in \{A,C,G,T\}$.
% {\color{red}SOMETHING ABOUT SLIDING WINDOW. One approach to a change dection problem is the use of a sliding window analysis that take a sldkfasldkfjas;dlfkj something something. It is sensitive to window size. Noise. etc}

The following is an outline of a Bayesian method for detecting change points, originally presented by Liu and Lawrence \cite{liu1999bayesian}. We define the change points $A_k$ to occur the first time a new die is used. That is
    \begin{equation}
        A_k = \sum_{\nu=0}^{k}C_\nu + 1
    \end{equation}
where $C_\nu$ is the number of times the $\nu$th dice was rolled, $C_0 = 0$ and $k = 1,\ldots,\kappa$. The quantities of interest are the number of change points $\kappa$ and their locations $\*A = (A_1,\ldots,A_\kappa)$. In this framework, $\kappa$ is chosen through model selection as the number of parameters changes with this. Continuing with the dice analogy, let $\Theta_i = (\theta_{k1},\ldots,\theta_{iD})$ represent the distribution of the dice used in the $i$th segment; that is $P(s_j = d) = \theta_{id}$ in segment $i$. With the assumptions that the segments are independent of one another given the change point positions, the likelihood for the model can be written as the product of the likelihood for each segment and the prior for the change point positions;
    \begin{equation}
        P(S,A|\Theta, \kappa) = \prod_{i=1}^{\kappa} P(S_{[A_{i-1},A_{i}}|\Theta_i)P(A|\kappa).
    \end{equation}
In this equation, $S_[i,j) = s_i\ldots s_{j-1}$. The joint distribution can then be written as
    \begin{equation}
        P(S,A,\Theta,\kappa) =  P(S,A|\Theta, \kappa)P(\kappa)P(\Theta)
    \end{equation}
under the a priori assumption that $\kapa$ and $\Theta$ are independent. From here, marginal distributions for the quantities of interest can be obtained and sampled from using MCMC, such as Gibbs sampling. 

Some limitation is in the model selection of the number of change points $\kappa$ and the idea that each segment as generated by different processes. 
Change Point by Keith~\cite{keith2006segmenting} is an extension of the model incorporating the comparative techniques from sequence alignment, including $\kappa$ into the inference as well as introducing the idea of allocating segments to classes of homogeneous properties, where non-adjacent segments may have been generated by the same underlying process~\cite{oldmeadow2009multiple}. The posterior distributions obtained in Change Point are on state spaces with differing dimension due to the inclusion of $\kappa$ into the inference. As such, conventional MCMC algorithms like MH and Gibbs will not work to sample from these posteriors, but the GGS works fine for such a situation~\cite{keith2004generalized}.
% {\color{red} Work on a general framework on how sequence segmentation wotks in Bayesian}

% Consider a sequence $A = A_1A_2,\ledots$

% Sequence change detection is the 